Table of contents
* [Introduction](#Introduction)
* [Hadoop Cluster](#Hadoop-Cluster)
* [Hive Project](#Hive-Project)
* [Improvements](#Improvements)


# Introduction
- Purpose of this project: help data processing team to process the data using Apache Hadoop  
- Evaluated Core Hadoop components, including MapReduce, HDFS, and YARN
- Provisioned a Hadoop Cluster using GCP
- Solved business problem using Apache Hive and Zeppelin Notebook

# Hadoop Cluster
![image](/hadoop/ClusterDiagram.png)
  - 1 master and 2 workers nodes
  - HDFS, YARN, Zeppelin, Hive (hive Server, hive metastore, RDBMS), etc.
- In this project I evaluated big data tools: MapReuce, YARN, HDFS, Hive, Zeppelin
- hardware specifications of each node: 2 CPUs, 12 GB RAM, 100 GB memory 

# Hive Project
- The purpose of this project was to practice using Apache Hive and process the real world data. Create different types of tables and see their effects on speed of the queries. Try to use different SerDe (lazySerDe and OpenCSVSerDe).  
- Post your Zeppelin Notebook screenshot here
	- Make sure your Notebook is nice and clean as hiring managers will visit your project
	- use `Full Page Screen Capture` chrome extention to capture a webpage as a picture

# Improvements
If you have more time, what would you improve?
- at least three improvements
